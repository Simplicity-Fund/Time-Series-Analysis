# Time Series Forecasting Models

## Autoregressive (AR) Model
The AutoRegressive (AR) model assumes that the future value of a time series depends on its past values. The "order" of the AR model indicates how many past values (lags) are used to predict the current value.

Equation:

$$
Y_t = c + φ_1Y_{t-1} + φ_2Y_{t-2} + \dots + φ_pY_{t-p} + ε_t
$$

- $Y_t$ : current value at time t
- c : constant term
- $φ_t$ : coefficients (weights) for past values (lags)
- $ε_t$ : error term (white noise)
- p : the order of the AR model (number of lagged terms)

### Steps to Build an AR Model

- **Step 1**: Check for stationarity (using ADF test) because AR models work on stationary data.
- **Step 2**: Choose the lag order (using PACF plot).
- **Step 3**: Estimate the model and evaluate residuals.

## Moving Average (MA) Model

The **Moving Average (MA)** model predicts future values based on past **errors** (residuals). It models the relationship between the current value and past prediction errors.
Equation:

$$
Y_t = c + θ_1ε_{t-1} + θ_2ε_{t-2} + \dots + θ_pε_{t-q} + ε_t
$$

- $θ_t$ : coefficients for the error terms
- $ε_t$ : error term at time t (random shock)
- q : the order of the MA model (number of lagged residuals)

### Steps to Build an MA Model:

- **Step 1**: Check for stationarity.
- **Step 2**: Choose the lag order using the **ACF plot** (ACF helps for MA).
- **Step 3**: Fit and evaluate the model.

## ARMA Model (AutoRegressive Moving Average)

The **ARMA** model combines both **AR** and **MA** components, meaning it accounts for both past values and past residuals.
Equation:

$$
Y_t = c + \sum_{i=1}^{p}φ_iY_t + \sum_{j=1}^{q}θ_iε_t + e_t
$$

## ARIMA (AutoRegressive Integrated Moving Average)

The **ARIMA** model generalizes ARMA by adding **differencing** to handle non-stationary data. It’s a powerful tool for forecasting.
ARIMA is represented as **ARIMA(p, d, q)**:

- **p**: Number of AR terms (lagged values)
- **d**: Differencing order to make the series stationary
- **q**: Number of MA terms (lagged errors)

The differencing step makes the time series stationary by removing trends. This is especially useful for stock prices or time series with trends.

## Seasonal ARIMA (SARIMA)

For time series with clear **seasonality**, ARIMA needs to be extended to account for seasonal patterns. This is where **SARIMA** comes in, represented as **SARIMA(p, d, q) (P, D, Q, s)**:

- **p, d, q**: Regular ARIMA terms (non-seasonal part)
- **P, D, Q**: Seasonal terms for AR, differencing, and MA
- **s**: Seasonal period (e.g., 12 for monthly data with yearly seasonality)

## Evaluating Forecasting Models

Once you have fitted your models, you can evaluate their performance using a variety of techniques and metrics to understand how well the model forecasts. Here's a comprehensive approach to evaluating the forecasting models:

### **In-sample Evaluation**:

In-sample evaluation assesses the model's performance on the training data (i.e., the data used to fit the model). While this can give you some insight into how well the model captures patterns, it's generally better to focus on out-of-sample evaluation to avoid overfitting.

**Metrics for In-sample Evaluation:**

- **Log-likelihood (LL)**:
    
    The log-likelihood is a measure of how likely it is that the observed data was generated by the model given the model's parameters. It essentially represents the likelihood of observing the actual values from the model's distribution.
    
    - When fitting an forecasting model, it assumes that the errors (or residuals) follow a specific probability distribution, often Gaussian (normal distribution).
    - The **likelihood function** is the probability of the observed data given the parameters of the model.
    - The **log-likelihood** is simply the natural logarithm of this likelihood function. Logarithms are used for computational stability, as likelihood values can become extremely small for large datasets.
    
    **Higher log-likelihood** values indicate that the model is a better fit for the data, meaning the model has higher probabilities for the observed data points under the fitted parameters.
    
    However, a high log-likelihood on its own does not always mean the model is good, as it can overfit the data by using too many parameters. 
    
    Formula:
    
    If the model has parameters θ and the observed data is X, the likelihood function $L(θ|X)$ describes the probability of the observed data given the model. The log-likelihood is given by:
    
    $$
    LL=Log(L(θ|X))
    $$
    
- **AIC (Akaike Information Criterion)**:
    
    The AIC is a criterion that balances the goodness of fit (based on the log-likelihood) with model complexity (penalizing the number of parameters). It’s designed to prevent overfitting by penalizing models with too many parameters, which could lead to excellent in-sample performance but poor out-of-sample predictions.
    
    **Lower AIC values are better.** A lower AIC indicates a model that has a good balance between accuracy and simplicity.
    
    The **AIC penalizes overfitting** by increasing the value if more parameters are added to the model unnecessarily.
    
    **Relative comparison**: AIC is most useful when comparing different models. It’s not an absolute measure of quality but rather a relative one. If you're comparing two or more models, the model with the lowest AIC is generally preferred.
    
    Formula:
    
    $$
    AIC = 2k - LL
    $$
    
    where k is  the number of estimated parameters in the model (including the intercept).
    

- **BIC (Bayesian Information Criterion)**:
    
    The BIC is similar to the AIC but penalizes the model complexity (number of parameters) more strongly. This makes the BIC more conservative when selecting models, favoring simpler models unless the increase in log-likelihood is substantial.
    
    Like the AIC, **lower BIC values are better**.
    
    **More conservative**: Since the BIC penalizes models more for having extra parameters it will often select simpler models compared to AIC, especially when the sample size n is large.
    
    In general, BIC will prefer models that generalize better to unseen data, while AIC might favor slightly more complex models that fit the training data better.
    
    Formula:
    
    $$
    BIC=klog(n)-2LL
    $$
    
    where k is the number of parameters in the model, and n the number of data points (sample size).
    

When the value of LL, AIC, and BIC are very large numbers, that’s not a bad think necessarily. These values are dependent on the size of your dataset and the complexity of your model. 

- **LL (Log-likelihood)** will naturally become more negative as the number of observations increases or if there is more complexity in the data. For a larger dataset or more variance in the data, the log-likelihood function results in lower values.
- **AIC and BIC** increase with both the number of parameters in the model and the sample size. A large dataset combined with a complex model can easily lead to high AIC and BIC values.

### **Out-of-sample Evaluation (on Test Data)**:

This is the most critical evaluation step. It measures how well the model forecasts future data (or unseen data).

**Steps for Out-of-sample Evaluation:**

1. **Generate Forecasts**: Use the fitted model to forecast values beyond the training set.
2. **Compare Forecasts to Actual Values**: Compare the predicted values with the actual values from the test set.

**Metrics for Forecast Evaluation:**

- **Mean Absolute Error (MAE)**: Average of the absolute errors between actual and predicted values.
- **Mean Squared Error (MSE)**: Average of the squared errors between actual and predicted values.
- **Root Mean Squared Error (RMSE)**: Square root of MSE, which penalizes larger errors more heavily.
- **Mean Absolute Percentage Error (MAPE)**: Average percentage difference between predicted and actual values.

### **Residual Analysis**:

Analyzing the residuals (the difference between actual and predicted values) is crucial to check if the model has captured the patterns in the data adequately.

- **Plot the Residuals**: Residuals should ideally behave like white noise (uncorrelated and normally distributed with a mean of zero). You can plot them using:
- **Check for Autocorrelation**: You can check if the residuals are autocorrelated by plotting the **ACF (Autocorrelation Function)**.
    
    If the residuals are autocorrelated, it may indicate that the model has not fully captured the time-dependent structure of the data.
    
- **Q-Q Plot**: A quantile-quantile plot helps assess if the residuals are normally distributed.

Always first depend of residual analysis:

AIC and BIC are **relative** indicators used for model comparison. Here’s why your model might still be well-fitted:

- **Residuals are behaving well**: If the residuals show no autocorrelation and are normally distributed, this means your model has likely captured the important patterns and seasonality in the data. This is the most important sign of a good model.
- **AIC and BIC are not absolute performance measures**: They help in comparing models. A large AIC/BIC does not mean the model is bad; it just reflects the fit and complexity trade-off. If you were to fit another ARIMA/SARIMA model to the same data, you would look for a **lower AIC/BIC** to identify a better model.
- **Dataset characteristics**: If your dataset has a lot of observations, the model naturally becomes more complex because there are more data points to explain. In that case, you would expect higher AIC and BIC values. However, if the residuals are behaving well, the model is effectively capturing the underlying patterns in your data.